{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y5LCBS-oCd4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "class TicTacToe:\n",
        "  def __init__(self , player1 , player2):\n",
        "    self.player1 = player1\n",
        "    self.player2 = player2\n",
        "    self.board = [' '] * 9\n",
        "    self.player1_turn = random.choice([True , False])\n",
        "\n",
        "  def check_full(self , ):\n",
        "    emptys = [x for x in self.board if x == ' ']\n",
        "    if len(emptys) == 0:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def print_board(self , ):\n",
        "    b  = self.board\n",
        "    board = f\"\"\"\n",
        "        {b[0]} | {b[1]} | {b[2]}\n",
        "        --|---|---\n",
        "        {b[3]} | {b[4]} | {b[5]}\n",
        "        --|---|---\n",
        "        {b[6]} | {b[7]} | {b[8]}\n",
        "    \"\"\"\n",
        "\n",
        "    print (board)\n",
        "\n",
        "\n",
        "\n",
        "  def check_winner(self , char):\n",
        "    b = self.board\n",
        "    winner = False\n",
        "    conditions = (\n",
        "        (0 , 1 , 2) , (3 , 4 , 5) , (6 , 7 , 8) ,\n",
        "        (0 , 3 , 6) , (1 , 4 , 7) , (2 , 5 , 8) ,\n",
        "        (0 , 4 , 8) , (2 , 4 , 6)\n",
        "    )\n",
        "\n",
        "\n",
        "    for each_conditions in conditions:\n",
        "      i1 , i2 , i3 = each_conditions\n",
        "      if char == b[i1] == b[i2] == b[i3]:\n",
        "        winner = True\n",
        "        return winner\n",
        "\n",
        "    return winner\n",
        "\n",
        "  def play(self , ):\n",
        "    self.player1.new_game()\n",
        "    self.player2.new_game()\n",
        "\n",
        "\n",
        "    while True:\n",
        "      if self.player1_turn:\n",
        "        player = self.player1\n",
        "        other_player = self.player2\n",
        "      else:\n",
        "        player = self.player2\n",
        "        other_player = self.player1\n",
        "      self.print_board()\n",
        "      #Ask for move\n",
        "      if player.player_type == 'human':\n",
        "        print(f'{player.name} Turn')\n",
        "        self.print_board()\n",
        "\n",
        "\n",
        "      move = player.make_move(self.board)\n",
        "\n",
        "\n",
        "      #check validity\n",
        "      if (move < 0 or move > 8) or (self.board[move] != ' '):\n",
        "        player.reward(-0.25 , self.board)\n",
        "        print('Invalid Move!!!')\n",
        "        continue\n",
        "      #put char\n",
        "      self.board[move] = player.char\n",
        "      #check winner\n",
        "      win =self.check_winner(player.char)\n",
        "      if win :\n",
        "        player.winner()\n",
        "        player.reward(5 , self.board)\n",
        "        other_player.reward(-5 , self.board)\n",
        "        self.print_board()\n",
        "        break\n",
        "      #check full(draw)\n",
        "      full = self.check_full()\n",
        "      if full:\n",
        "        print('Game ended as draw')\n",
        "        player.reward(0.5 , self.board)\n",
        "        other_player.reward(0.5 , self.board)\n",
        "        self.print_board()\n",
        "        break\n",
        "      #swap turn\n",
        "      player.reward(0.25 , self.board)\n",
        "      self.player1_turn = not self.player1_turn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Player:\n",
        "  def __init__(self , name , char , player_type = 'human'):\n",
        "    self.name = name\n",
        "    self.char = char\n",
        "    self.player_type = player_type\n",
        "\n",
        "\n",
        "  def make_move(self , board):\n",
        "    move = input('Make your move: ')\n",
        "    return int(move)\n",
        "\n",
        "\n",
        "  def new_game(self,):\n",
        "    print(f'{self.name} is {self.char}')\n",
        "\n",
        "\n",
        "  def winner(self , ):\n",
        "    print(f'{self.name} is winner')\n",
        "\n",
        "\n",
        "  def reward(self , reward_value , board):\n",
        "    print(f'{self.name} gets {reward_value}')"
      ],
      "metadata": {
        "id": "-aGb66Os895s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AI(Player):\n",
        "  def __init__(self , name , char , epsilon = 1 , gamma = 0.9 , alpha = 0.01):\n",
        "    super().__init__(name , char , player_type = 'AI')\n",
        "    self.epsilon = epsilon # Espilon Greddy\n",
        "    self.gamma = gamma # Discount Factor\n",
        "    self.alpha = alpha # Learing Rate\n",
        "    self.q_table = {}\n",
        "\n",
        "  def get_Q(self , state , action):\n",
        "    if self.q_table.get((state , action)) == None:\n",
        "      self.q_table[(state , action)] = 5\n",
        "    return self.q_table[(self , action)]\n",
        "\n",
        "  def aviable_moves(self , board):\n",
        "    return [x for x in range(9) if board[x] == ' ' ]\n",
        "\n",
        "  def q_learn(self , state , action , reward , new_state):\n",
        "    '''\n",
        "    new_q_value = prev_q_value + alpha * (reward + gamma * max_q - prev_q_value)\n",
        "    '''\n",
        "    prev_q_value = self.get_Q(state , action) # make this reward\n",
        "    aviable_actions = self.aviable_moves(state) # make this reward\n",
        "    aviable_q_values = [self.get_Q(new_state , action ) for action in aviable_actions]\n",
        "    max_q = max(aviable_q_values)\n",
        "    self.q_table[(state , action)] = prev_q_value + self.alpha * (reward + self.gamma * max_q - prev_q_value)\n",
        "\n",
        "  def reward(self, reward_value , board):\n",
        "    self.q_learn(self.prev_board ,\n",
        "                 self.prev_action ,\n",
        "                 reward_value ,\n",
        "                 tuple(board))\n",
        "\n",
        "  def make_move(self , board):\n",
        "    # Save old state\n",
        "    self.prev_board = tuple(board)\n",
        "    aviable_action = self.aviable_moves(board)\n",
        "    #Select Random Action(Exploration)\n",
        "    if random.random() < self.epsilon:\n",
        "      action_taken = random.choice(aviable_action)\n",
        "      self.prev_action = action_taken\n",
        "      self.epsilon *= 0.9999999999 # Epsilon decay\n",
        "      return action_taken\n",
        "\n",
        "    #Selecting action with max Q-Vlalue(Exploitation)\n",
        "    q_values = []\n",
        "    for actions in aviable_action:\n",
        "      q_values.append(self.get_Q(board , actions))\n",
        "    max_q_value = max(q_values)\n",
        "    index = q_values.index(max_q_value)\n",
        "    action_taken = aviable_action[index]\n",
        "    return action_taken"
      ],
      "metadata": {
        "id": "eD29w93r9VDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = AI('P-One' , 'X')\n",
        "p2 = AI('p-Two' , 'O')"
      ],
      "metadata": {
        "id": "KX5JXHnJ9CpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqHjDvNaPIes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f'\\nEpisode {i + 1}')\n",
        "  game = TicTacToe(p1 , p2)\n",
        "  game.play()"
      ],
      "metadata": {
        "id": "Oi_cvRHARMiU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "outputId": "6e262a40-b23b-45a1-f705-d158a006cd6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Episode 1\n",
            "P-One is X\n",
            "p-Two is O\n",
            "\n",
            "          |   |  \n",
            "        --|---|---\n",
            "          |   |  \n",
            "        --|---|---\n",
            "          |   |  \n",
            "    \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7bf13fa6a97c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nEpisode {i + 1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicTacToe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-11f5547db2cf>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0;31m#swap turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1_turn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer1_turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-479432fe62de>\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self, reward_value, board)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_value\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     self.q_learn(self.prev_board ,\n\u001b[0m\u001b[1;32m     29\u001b[0m                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_action\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                  \u001b[0mreward_value\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-479432fe62de>\u001b[0m in \u001b[0;36mq_learn\u001b[0;34m(self, state, action, reward, new_state)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mnew_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_q_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_q\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_q_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     '''\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprev_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make this reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0maviable_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maviable_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make this reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0maviable_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maviable_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-479432fe62de>\u001b[0m in \u001b[0;36mget_Q\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maviable_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (<__main__.AI object at 0x7c5ec7135ff0>, 1)"
          ]
        }
      ]
    }
  ]
}